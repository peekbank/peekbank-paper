---
title: "Peekbank: Exploring children's word recognition through an open, large-scale repository for developmental eye-tracking data"
bibliography: peekbank_cogsci2021.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Martin Zettersten (martincz@princeton.edu)} \\ Department of Psychology, South Dr \\ Princeton, NJ 08540 USA
    \AND {\large \bf Tian Linger Xu (txu@iu.edu)} 
    \AND {\large \bf Stephan Meylan (smeylan@mit.edu)} 
    \AND {\large \bf Mika Braginsky (mikabr@mit.edu)} 
    \AND {\large \bf George Kachergis (kachergis@stanford.edu)} 
    \AND {\large \bf Molly Lewis (mollyllewis@gmail.com)} 
    \AND {\large \bf Claire Bergey (cbergey@uchicago.edu)} 
    \AND {\large \bf Naiti S. Bhatt (nbhatt@hmc.edu)} 
    \AND {\large \bf Veronica Boyce (vboyce@stanford.edu)} 
    \AND {\large \bf Jessica Mankewitz (jmankewitz@stanford.edu)}
    \AND {\large \bf Bria Long (bria@stanford.edu)} 
    \AND {\large \bf Benny deMayo (bdemayo@princeton.edu)} 
    \AND {\large \bf Daniel Yurovsky (yurovsky@stanford.edu)} 
    \AND {\large \bf Annissa N. Saleh (ans638@nyu.edu)} 
    \AND {\large \bf Sarp Uner (sarp.uner@duke.edu)} 
    \AND {\large \bf Alexandra Carstensen (abcarstensen@stanford.edu)} 
    \AND {\large \bf Rose M. Schneider (roschnei@ucsd.edu)} 
    \AND {\large \bf Angeline Sin Mei Tsui (astsui@stanford.edu)}  
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)}}

abstract: >
    Word recognition -- the ability to rapidly recognize words and link them to referents in context -- is central to children's early language development. Children's word recognition is typically studied in the looking-while-listening paradigm, which measures infants' fixation of a target object (vs. a distracter) after hearing a target label. We present a large-scale, open database of infant and toddler eye-tracking data from looking-while-listening tasks. The goal of this effort is to address theoretical and methodological challenges in measuring infant vocabulary development. We present two analyses of the current database (N=1,233): (1) capturing age-related changes in infants' word recognition while generalizing across item-level variability and (2) assessing how a central methodological decision -- selecting the time window of analysis -- impacts the reliability of measurement. Future efforts will expand the scope of the current database to advance our understanding of participant-level and item-level variation in children's vocabulary development.
    
keywords: >
    word recognition; eye-tracking; database; vocabulary development; looking-while-listening
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(here)
```

```{r}
#load(file = here("data","aoi_data_joined.Rds"))
```


# Introduction

Across their first years of life, children learn words in their native tongues at a rapid pace [@frank2021]. A key part of the word learning process is children’s ability to rapidly process words and link them to relevant meanings - often referred to as word recognition. Developing word recognition skills builds a foundation for children’s language development and is predictive of later linguistic and general cognitive outcomes [@Bleses2016;@Marchman2018].

Word recognition is traditionally studied in the “looking-while-listening” paradigm [alternatively referred to as the intermodal preferential looking procedure; @Fernald2008;@Hirsh-Pasek1987]. 
In such studies, infants listen to a sentence prompting a specific referent (e.g., *Look at the dog!*) while viewing two images on the screen (e.g., an image of a dog - the target image - and an image of a duck - the distractor image). 
Infants’ word recognition is measured in terms of how quickly and accurately they fixate the correct target image after hearing its label. 
Studies using this design have contributed to our understanding of a wide range of questions in language development, including infants’ early noun knowledge, phonological representations of words, prediction during language processing, and individual differences in language development [@Bergelson2012a;@Golinkoff2013;@Lew-Williams2007;@Marchman2018;@Swingley2000].

While the looking-while-listening paradigm has been highly fruitful in advancing understanding of early word knowledge, fundamental questions remain both about the trajectory of children's word recognition ability and the nature of the method itself. One central question is how to tease apart variability due to specific items (words) and variability due to participants across development. Most studies of infant word recognition focus on generalizing performance across participants, and are constrained in their ability to provide generalizations across words (items). Generalizing behavior on the level of both items and participants simultaneously is difficult in the context of a solitary study, especially given practical constraints on the number of trials (and consequently items) tested within a given infant. One key to meeting this challenge is having sufficiently large datasets to account for and explain variability in word recognition on the item level.

A second question relates to evaluating methodological best-practices. 
In particular, many fundamental analytic decisions vary substantially across studies, and different decisions may lead to different inferences about children's word recognition. 
For example, researchers vary in how they  select time windows for analysis, transform the dependent measure of target fixations, and model the time course of word recognition [@Csibra2016;@Fernald2008;@Huang2020]. 
This problem is made more complex by the fact that many of these decisions likely depend on a variety of design-related and participant-related factors (e.g., infant age).
Establishing best practices therefore requires a large database of infant word recognition studies varying across such factors, in order to independently test the potential consequences of methodological decisions on the interpretation of study results.

<!-- Peekbank: A large-scale database of looking-while-listening-studies -->
What these two questions share is that they are difficult to answer at the scale of a single study. 
To address this challenge, we introduce *Peekbank*, a flexible and reproducible interface to an open database of developmental eye-tracking studies.
The Peekbank project (a) collects a large set of eye-tracking datasets on children’s word recognition, (b) introduces a data format and processing tools for standardizing eye-tracking data across data sources, and (c) provides an API for accessing and analyzing the database. 
In the current paper, we give a brief overview of the key components of the project and some initial demonstrations of its utility in advancing theoretical and methodological questions in the study of children’s early language. 
We report two analyses using the database and associated tools (N=1,233): (1) a growth-curve analysis modeling age-related changes in infants' word recognition while generalizing across item-level variability and (2) a multiverse-style analysis of how a central methodological decision -- selecting the time window of analysis -- impacts inter-item reliability.

# Methods

## Database Framework

```{r fig_framework_overview, fig.env = "figure", fig.align = "center", fig.height=4.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Overview of the peekbank data ecosystem. Peekbank libraries and tools are highlighted in green."}
img <- png::readPNG(here("figures","peekbankflowchartv4.png"))
grid::grid.raster(img)
```

The *Peekbank* data framework consists of three libraries that take in raw experimental datasets, populate  a relational database, and provide an API interface (Fig. \ref{fig:fig_framework_overview}).
The \texttt{peekds} library (for the R language) helps researchers convert and validate existing datasets to use the relational format used by the database. 
The \texttt{peekbank} library (Python) creates a database with the relational schema and populates it with the standardized datasets produced by \texttt{peekds}.
The database is implemented in MySQL, an industry standard relational database, which may be accessed by a variety of programming languages over the internet.
The \texttt{peekbankr} library (R) provides an application programming interface, or API, that provides high-level abstractions to help researchers run common analysis tasks on the database. 
<!--
```{r fig_schema, fig.env = "figure", fig.align = "center", fig.width=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Data schema for the peekbank database."}
img <- png::readPNG(here("figures","schema.png"))
#https://docs.google.com/presentation/d/1jZfGuTZFo5k4rcObYWJhdu94H7BnPvKYoGKJzQBBsUs/edit#slide=id.g7961a2384e_0_3
grid::grid.raster(img)
```
-->

## Data Format and Processing

One of the main challenges in compiling a large-scale eye-tracking dataset is the lack of a shared re-usable data format among labs conducting individual experiments. Researcher conventions for exporting and structuring data vary, as do the technical specifications of different devices, rendering the task of integrating datasets from different labs and data sources difficult. We developed a common, tidy format for the eye-tracking data in *Peekbank* to ease the process of conducting cross-dataset analyses [@Wickham2019]. The schema of the database <!--(Fig. \ref{fig:fig_schema})--> is sufficiently general to handle heterogeneous datasets, including both manually coded and automated eye-tracking data.

During data import, raw eye-tracking datasets are processed to conform to the *Peekbank* data schema. The centerpiece of the schema is the aoi_timepoints table (Fig. \ref{fig:fig_framework_overview}), which records whether participants looked to the target or the distracter stimulus at each timepoint of a given trial. Additional tables track information about data sources, participant characteristics, trial characteristics, stimuli, and raw eye-tracking data information. In addition to unifying the data format, we conduct several additional pre-processing steps to facilitate analyses across datasets, including resampling observations to a common sampling rate (40 Hz) and normalizing time relative to the onset of the target label.

## Current Data Sources

```{r xtable, num.cols.cap=1, results="asis"}
load(file = here("data","dataset_info.Rds"))

dataset_name_mapping <- read_csv(here("data","dataset_name_mapping.csv"))

dataset_unique_subj <- dataset_info %>%
  distinct(subject_id,sex)

summarize_datasets <- dataset_info %>%
  left_join(dataset_name_mapping) %>%
  group_by(dataset_rename) %>%
  summarize(
    #num_admin=length(unique(administration_id)),
    num_subj=length(unique(subject_id)),
    avg_age=mean(age,na.rm=T),
    method=unique(coding_method)[1]
  ) %>%
  mutate(
    method=case_when(
      method=="manual gaze coding" ~ "manual coding",
      TRUE ~ method)
  ) %>%
  arrange(dataset_rename)

tab1 <- xtable::xtable(summarize_datasets, digits=c(1), 
                       caption = "Overview over the datasets in the current database.")

names(tab1) <- c("Dataset Name", "N","Mean Age", "Method")

print(tab1, type="latex", comment = F, table.placement = "H",include.rownames=FALSE, size="\\fontsize{9pt}{10pt}\\selectfont")
```

The database currently includes `r length(summarize_datasets$dataset_rename)` looking-while-listening datasets comprising N=`r sum(summarize_datasets$num_subj)` total participants, with `r min(summarize_datasets$num_subj)` to `r max(summarize_datasets$num_subj)` participants per dataset (Table 1). Most datasets (10 out of 11 total) consist of data from monolingual native English speakers. They span a wide age spectrum with participants ranging from `r min(dataset_info$age,na.rm=TRUE)` to `r max(dataset_info$age,na.rm=TRUE)` months of age, and are balanced in terms of gender (`r round(mean(dataset_unique_subj$sex=="female"),2)*100`% female). The studies in the current database vary across a number of dimensions related to design and methodology. 
The database includes studies using both manually coded video recordings and automated eye-tracking methods (e.g., Tobii, EyeLink) to measure children's gaze behavior. 
Most studies focused on testing familiar items, but the database also includes studies with novel pseudowords.

```{r peekbank_item_vis, fig.env = "figure*", fig.pos = "h", fig.width=6.5, fig.height=3.9, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Item-level variability in proportion target looking within each dataset (chance=0.5). Time is centered on the onset of the target label (vertical line). Colored lines represent specific target labels. Black lines represent smoothed average fits based on a general additive model using cubic splines."}
img <- png::readPNG(here("figures","peekbank_item_vis.png"))
grid::grid.raster(img)
```

# Results

## General descriptives and item variability

```{r xtable2, num.cols.cap=1, results="asis"}
dataset_means <- read_csv(here("data","dataset_means_prop_looking.csv"))
dataset_name_mapping <- read_csv(here("data","dataset_name_mapping.csv"))

dataset_means <- dataset_means %>%
  left_join(dataset_name_mapping) %>%
  select(dataset_rename,unique_items,prop_looking,prop_looking_lower_ci,prop_looking_upper_ci) %>%
  arrange(dataset_rename) %>%
  mutate(ci=paste0("[",round(prop_looking_lower_ci,2),", ",round(prop_looking_upper_ci,2),"]")) %>%
  select(-prop_looking_lower_ci,-prop_looking_upper_ci)

tab2 <- xtable::xtable(dataset_means, digits=c(0,0,0,2,0), 
                       caption = "Average proportion target looking in each dataset.")

names(tab2) <- c("Dataset Name", "Unique Items","Prop. Target", "95% CI")
print(tab2, type="latex", comment = F, table.placement = "H",include.rownames=FALSE, size="\\fontsize{9pt}{10pt}\\selectfont")

participant_means_novelty <- read_csv(here("data","participant_means_prop_looking_novelty.csv"))

overall_novelty <- participant_means_novelty %>%
  group_by(stimulus_novelty) %>%
  summarize(
    N=n(),
    prop_looking=mean(avg_prop_looking,na.rm=TRUE),
    prop_looking_ci=qt(0.975, N-1)*sd(avg_prop_looking,na.rm=TRUE)/sqrt(N),
    prop_looking_lower_ci=prop_looking-prop_looking_ci,
    prop_looking_upper_ci=prop_looking+prop_looking_ci
  )

# participant_means <- read_csv(here("data","participant_means_prop_looking.csv"))
# #fit age prediction model
# m <- lmer(avg_prop_looking ~ age+(1|dataset_name),data=by_participant_means)
```

In general, participants demonstrated robust, above-chance word recognition in each dataset (with chance being 0.5). 
Table 2 shows the average proportion of target looking within a standard critical window of 300 - 2000ms after the onset of the label for each dataset [@Swingley2000]. 
The number of unique target labels and their associated accuracy vary widely across datasets (Figure \ref{fig:peekbank_item_vis}). 
Proportion target looking was generally higher for familiar target labels (M = `r round(filter(overall_novelty,stimulus_novelty=="familiar")$prop_looking,3)*100`%, 95% CI = [`r round(filter(overall_novelty,stimulus_novelty=="familiar")$prop_looking_lower_ci,3)*100`%, `r round(filter(overall_novelty,stimulus_novelty=="familiar")$prop_looking_upper_ci,3)*100`%]) than for novel target labels learned during the experiment (M = `r round(filter(overall_novelty,stimulus_novelty=="novel")$prop_looking,3)*100`%, 95% CI = [`r round(filter(overall_novelty,stimulus_novelty=="novel")$prop_looking_lower_ci,3)*100`%, `r round(filter(overall_novelty,stimulus_novelty=="novel")$prop_looking_upper_ci,3)*100`%]). 

## Predicting Age-Related Changes While Generalizing Across Items

```{r age_gca, fig.env = "figure", fig.env = "figure*", fig.pos = "h", fig.width=6.5, fig.height=4.33, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Growth curve models of proportion target looking during the critical target window at each age range (in months). (A) Mean empirical word recognition fit. (B) Population-level estimates."}
img <- png::readPNG(here("figures","age_gca.png"))
grid::grid.raster(img)
```

Developmental changes in word recognition have been a central issue since early investigations of eye-tracking techniques [@fernald1998]. Children's speed and accuracy of word recognition increases across early childhood, yet measuring these increases presents an item selection puzzle for researchers: Words that are appropriate for an 18-month-old will be too easy for a three-year-old; those that are appropriate for a three-year-old will be difficult the 18-month-old. Failure to choose appropriate test items can even lead to spurious conclusions about development [@peter2019]. 

This issue is familiar in psychometrics: test developers interested in measuring across a wide range of a particular latent ability must choose items appropriate for different abilities. One solution is to use data from a bank of questions that have been taken by test-takers of a range of abilities, and then use item-response theory models to create different test versions appropriate for different ability ranges [@embretson2000]. Such tests can then be used to extract estimates of developmental change that are independent of individual tests and their particular items. 

*Peekbank* provides the appropriate data for estimating these item-independent developmental changes and designing age-appropriate tests in the future. Here we show a proof of concept by providing an estimate of the item-independent growth of word recognition accuracy across development. We take advantage of the equivalence between item response theory and linear mixed effects models [LMMs; @de-boeck2011], using LMMs to model the trajectory of word recognition across age. We follow the approach of Mirman [-@Mirman2014] and use growth curve LMMs to predict the timecourse of recognition. Specifically, we predicted children's proportion of target looking during an early window of time (0 - 1500ms, chosen to avoid modeling declines in looking later in the trial), using an empirical logit transform on the proportion of target looking to allow the use of linear (rather than logistic) regression models. Our predictors were time after word onset and age, and we additionally included polynomial functions of time (up to fourth order) and quadratic effects of age, as well as their interactions. We subtracted all intercepts to force fits to start at a baseline of 0 (chance performance) at time zero. As a random effect structure, we included by-item, by-subject, and by-dataset random intercepts; though a larger random effect structure could be justified by the data, the size of the dataset precluded fitting these. 

Figure \ref{fig:age_gca} depicts the results of this analysis. Panel A shows the mean empirical word recognition curves for four age groups, along with fitted model performance. Although model fits are acceptable, developmental change appears irregular - for example, 12--24 month-olds show slightly earlier recognition than 24--36 month-olds. This pattern is an artifact of averaging across datasets with substantially different items and structures. Panel B shows model predictions for the population level of each random effect -- our best estimates of the latent ability structure. Here we see continuous increases in both speed (point at which the curve rises) and accuracy (asymptote of the curve) across ages, though this developmental trend decelerates [consistent with other work on reaction time development; @frank2016b; @kail1991]. This proof of concept suggests that *Peekbank* can be used to model developmental change over multiple years, overcoming the limitations of individual datasets.

## Time Window Selection

```{r time_window, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3.75, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Participants' average inter-item correlation for proportion of looking time to familiar targets, as a function of window start time and end time, with each facet showing a different age group. More positive (red) correlations are more desirable, and blue/white represent start/end time combinations that researchers should avoid."}
img <- png::readPNG(here("figures","interitem_cors_window_analysis.png"))
grid::grid.raster(img)
```

In our second analysis, we addressed a common analytic decision facing researchers: how to summarize timecourse data into a single measure of accuracy. 
Taking a similar approach to that of @Peelle2020, we conducted a multiverse-style analysis considering possible time windows researchers might select.
Our multiverse analysis focuses on the reliability of participants' response to familiar words by measuring the subject-level inter-item correlation (IIC) for proportion of looking at familiar targets. 
The time windows selected by researchers varies substantially in the literature, with some studies analyzing shorter time windows between 300 ms and 1800-2000 ms post-target onset [@Fernald2008;@Swingley2000], and others using longer time windows extending to approximately 3000-4000ms [especially with younger infants; e.g., @Bergelson2012a]. 
We thus examined a broad range of window start times ranging from 300 ms pre-target onset to 1500 ms post-target onset and window end times ranging from 0 ms to 4000 ms post-target onset. 
For each combination of window start time and end time with a minimum window duration of 50 ms, we calculated participants’ average inter-item correlation for proportion of looking at familiar targets (mean IIC). 
Since observations were unevenly distributed across the age range, and because children likely show a varying response to familiar items as they age (often motivating different window choices), we split our data into four age bins (12-24, 24-36, 36-48, and 48-60 months).
While it is an open question what space of possible windows will yield the greatest reliability, we expect to see low reliability (i.e. 0) in windows that start before target onset and in windows that end within 300 ms post-target onset, before participants are able to execute a response.
<!-- We removed the mahr_coartic dataset because it only included times up to 1800 ms post-target onset -->

Results from this multiverse analysis are shown in Figure \ref{fig:time_window}, where each colored pixel represents the mean IIC for proportion of looking to familiar targets for a specific combination of window start and end time. 
The analysis shows that IIC is positive (red) under a wide range of sensible window choices. 
IIC is relatively low however, especially for the youngest age group, suggesting that individual items carry only limited shared signal regarding children's underlying ability. 
It may be the case that even averaging many such trials does not yield highly reliable measures of individual differences, although some multi-trial paradigms are exceptions to this generalization [@Fernald2008].

Intriguingly, however, late end times and long overall window lengths show the greatest reliability. 
Shorter windows (e.g., 300-2000ms, as we used above) likely maximize absolute recognition performance by fitting the peak of the recognition curve, but simultaneously lower reliability by failing to include all relevant data. 
Especially for older children, the maximal IICs were found with windows that started between 500 and 1000ms and ended between 3000 and 4000ms, windows usually reserved for younger children. 
This finding is sensible from a psychometric perspective -- averaging more timepoints (even if some contain limited signal) increases reliability and reduces variation.
Thus, researchers interested in better measurement of individual variation or condition differences should consider using longer windows by default.
<!-- Moreover, there is some variation by age group in where the strongest ICCs are found (and in the overall strength of ICCs). <!-- discuss more? [GK: add mean +ICC per age?]) -->

<!-- What general recommendations follow from this analysis? -->
<!-- We minimally consider which start times and window lengths result in IICs of at least .01, noting that this is still a rather low target. -->
<!-- A window length of at least 1500 ms eliminated 94% of low ICCs, and this threshold combined with a start time of at least 300 ms eliminated all but 0.5% of low ICCs.  -->
<!-- A start time of 500 ms with a window length of at least 1500 ms resulted in no IICs < .01, and an average IIC of 0.08. -->
<!-- However, the overall strength of the IICs are generally weaker than might be desired (median = .05), with maximum values of 0.15 (reached only in 3-year-olds). -->
<!-- This suggests that researchers may be justified in using different start times and window sizes for different age ranges, likely due to the varying pull of familiarity and novelty as learners age. -->
<!-- [GK: add correlations between ICC and start time, end time, window length?] -->

# Discussion

Theoretical progress in understanding child development requires rich datasets, but collecting infant data is expensive, difficult, and time-intensive. 
Recent years have seen a growing effort to build open source tools and pool research efforts to meet the challenge of building a cumulative developmental science [@Bergmann2018; @TheManyBabiesConsortium2020]. 
The Peekbank project expands on these efforts by building an infrastructure for aggregating eye-tracking data across studies, with a particular focus on the looking-while-listening paradigm. 
This paper presents a preliminary illustration of some of the key theoretical and methodological questions that can be addressed using Peekbank data: generalizing across item-level variability in children's word recognition and providing data-driven guidance on methodological choices.

Our first analysis shows that Peekbank can be used to model item-independent changes in the speed and accuracy of word recognition across development. 
Children showed age-related increases in the speed of word recognition across one to five years of age, replicating and extending past foundational work [e.g., @fernald1998], while demonstrating that these word processing gains generalize across items and are not only attributable word-specific gains in processing speed. 
The second analysis demonstrates how Peekbank can be used to make principled, data-driven analytic decisions, specifically the decision of how to choose time windows for analyzing developmental eye-tracking data. 
In looking-while-listening studies, researchers often choose a relatively short time window of roughly 300-1800 or 2000 ms [@Fernald2008], with the justification that eye movements occurring after this window may no longer reflect fixations related to the target label [@Swingley2000]. 
Our results recommend that researchers consider increasing the size of the time window for analyzing target fixations (at least for familiar words) to maximize the consistent signal present in children's target fixations.

There are a number of critical limitations surrounding the current scope of the database. A key priority in future work will be to expand the size of the database. 
With 11 datasets currently available in the dataset, idiosyncrasies of particular designs and condition manipulations still have substantial influence on modeling results. 
Expanding the set of distinct datasets will allow us to increase the number of observations per item across datasets, allowing for more robust generalizations regarding participant- and item-level variability.
The current database is also limited by the relatively homogeneous background of its participants, both with respect to language (almost entirely monolingual native English speakers) and cultural background [all but one dataset comes from WEIRD environments; @Muthukrishna2020]. Increasing the diversity of participant backgrounds and languages will increase the scope of the generalizations we can form about child word recognition.

Finally, while the current database is mainly focused on studies of word recognition, the tools and infrastructure developed in the project can in principle be expanded to accommodate any eye-tracking paradigm used with infants and toddlers, opening up new avenues for insights into infant development. 
Infant looking has been at the core of many of the key advances in our understanding of infant cognition. 
Aggregating large datasets of infant looking behavior in a single, openly-accessible format promises to bring a fuller picture of infant cognitive development into view.

# Acknowledgements

We would like to thank the labs and researchers that have made their data publicly available in the database.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
